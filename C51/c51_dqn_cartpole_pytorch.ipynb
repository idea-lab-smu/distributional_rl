{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "#tihrd party\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constant\n",
    "SEED = 0\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "ADAM_EPS = 0.01 / BATCH_SIZE\n",
    "USE_TENSORBOARD = True\n",
    "\n",
    "# C51 hyperparameter\n",
    "V_MAX = 10\n",
    "V_MIN = -10\n",
    "N_ATOMS = 51\n",
    "DELTA_Z = (V_MAX - V_MIN) / (N_ATOMS - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda:', use_cuda)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# random seed\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# set tensorboard\n",
    "# tensorboard and tensorboardX must be installed.\n",
    "# pip install tensorboardX\n",
    "if USE_TENSORBOARD:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistrDQN(nn.Module):\n",
    "    def __init__(self, in_dim, n_actions, n_atoms):\n",
    "        super(DistrDQN, self).__init__()\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(in_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions * n_atoms)\n",
    "        )\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.register_buffer('support', torch.arange(V_MIN, V_MAX + DELTA_Z, DELTA_Z))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dense(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out).view(out.size(0), -1, N_ATOMS)\n",
    "        out = self.log_softmax(out)\n",
    "        probs = out.exp()\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(next_p, rewards, dones):\n",
    "    proj_p = np.zeros((BATCH_SIZE, N_ATOMS), dtype=np.float32)\n",
    "    for atom in range(N_ATOMS):\n",
    "        z = np.minimum(V_MAX, np.maximum(V_MIN, rewards + (V_MIN + atom * DELTA_Z) * 0.9))\n",
    "        b = (z - V_MIN) / DELTA_Z\n",
    "        l = np.floor(b).astype(np.int64)\n",
    "        u = np.ceil(b).astype(np.int64)\n",
    "\n",
    "        eq_mask = u == l\n",
    "        proj_p[eq_mask, l[eq_mask]] += next_p[eq_mask, atom]\n",
    "        ne_mask = u != l\n",
    "        proj_p[ne_mask, l[ne_mask]] += next_p[ne_mask, atom] * (u - b)[ne_mask]\n",
    "        proj_p[ne_mask, u[ne_mask]] += next_p[ne_mask, atom] * (b - l)[ne_mask]\n",
    "\n",
    "        if dones.any():\n",
    "            proj_p[dones] = 0.0\n",
    "            z = np.minimum(V_MAX, np.maximum(V_MIN, rewards[dones]))\n",
    "            b = (z - V_MIN) / DELTA_Z\n",
    "            l = np.floor(b).astype(np.int64)\n",
    "            u = np.ceil(b).astype(np.int64)\n",
    "\n",
    "            eq_mask = u == l\n",
    "            eq_dones = dones.copy()\n",
    "            eq_dones[dones] = eq_mask\n",
    "            if eq_dones.any():\n",
    "                proj_p[eq_dones, l] = 1.0\n",
    "\n",
    "            ne_mask = u != l\n",
    "            ne_dones = dones.copy()\n",
    "            ne_dones[dones] = ne_mask\n",
    "            if ne_dones.any():\n",
    "                proj_p[ne_dones, l] = (u - b)[ne_mask]\n",
    "                proj_p[ne_dones, u] = (b - l)[ne_mask]\n",
    "\n",
    "    return proj_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, tgt_net, rep_memory):\n",
    "    net.train()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LR, eps=ADAM_EPS)\n",
    "    \n",
    "    train_data = []\n",
    "    train_data.extend(random.sample(rep_memory, BATCH_SIZE))\n",
    "    \n",
    "    dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, pin_memory=use_cuda)\n",
    "    \n",
    "    for i, (s, a, r, _s, d) in enumerate(dataloader):\n",
    "        s_batch = s.to(device).float()\n",
    "        a_batch = a.to(device).long()\n",
    "        _s_batch = _s.to(device).float()\n",
    "        rewards = r.detach().cpu().numpy()\n",
    "        dones = d.detach().cpu().numpy().astype(np.bool)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _p_batch = tgt_net(_s_batch)\n",
    "            _weights = _p_batch * tgt_net.support\n",
    "            _q_batch = _weights.sum(dim=2)\n",
    "            _q_batch_np = _q_batch.cpu().numpy()[0]\n",
    "            _action_batch_np = np.argmax(_q_batch_np)\n",
    "            _p_best = _p_batch[range(BATCH_SIZE), _action_batch_np]\n",
    "            _p_best_np = _p_best.cpu().numpy()\n",
    "            \n",
    "        proj_p_np = projection(_p_best_np, rewards, dones)\n",
    "        proj_p = torch.tensor(proj_p_np).to(device).float()\n",
    "        \n",
    "        p_batch = net(s_batch)\n",
    "        p_acting = p_batch[range(BATCH_SIZE), a_batch.data]\n",
    "        \n",
    "        loss = -(proj_p * (p_acting + 1e-8).log()).sum(dim=1).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        assert loss == loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/MyProject/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "# make an environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(SEED)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# make two nerual network\n",
    "net = DistrDQN(obs_dim, n_actions, N_ATOMS).to(device)\n",
    "target_net = deepcopy(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Episode in  14 steps, reward 14.00\n",
      "  2 Episode in  12 steps, reward 12.00\n",
      "  3 Episode in  12 steps, reward 12.00\n",
      "  4 Episode in  20 steps, reward 20.00\n",
      "  5 Episode in  25 steps, reward 25.00\n",
      "  6 Episode in  10 steps, reward 10.00\n",
      "  7 Episode in  18 steps, reward 18.00\n",
      "  8 Episode in  12 steps, reward 12.00\n",
      "  9 Episode in  11 steps, reward 11.00\n",
      " 10 Episode in  15 steps, reward 15.00\n",
      " 11 Episode in  56 steps, reward 56.00\n",
      " 12 Episode in  18 steps, reward 18.00\n",
      " 13 Episode in  11 steps, reward 11.00\n",
      " 14 Episode in  23 steps, reward 23.00\n",
      " 15 Episode in  70 steps, reward 70.00\n",
      " 16 Episode in  10 steps, reward 10.00\n",
      " 17 Episode in  16 steps, reward 16.00\n",
      " 18 Episode in  17 steps, reward 17.00\n",
      " 19 Episode in  20 steps, reward 20.00\n",
      " 20 Episode in  17 steps, reward 17.00\n",
      " 21 Episode in  11 steps, reward 11.00\n",
      " 22 Episode in  29 steps, reward 29.00\n",
      " 23 Episode in  17 steps, reward 17.00\n",
      " 24 Episode in  36 steps, reward 36.00\n",
      " 25 Episode in  39 steps, reward 39.00\n",
      " 26 Episode in  22 steps, reward 22.00\n",
      " 27 Episode in  12 steps, reward 12.00\n",
      " 28 Episode in  32 steps, reward 32.00\n",
      " 29 Episode in  22 steps, reward 22.00\n",
      " 30 Episode in  15 steps, reward 15.00\n",
      " 31 Episode in  36 steps, reward 36.00\n",
      " 32 Episode in  22 steps, reward 22.00\n",
      " 33 Episode in  13 steps, reward 13.00\n",
      " 34 Episode in  23 steps, reward 23.00\n",
      " 35 Episode in  14 steps, reward 14.00\n",
      " 36 Episode in  32 steps, reward 32.00\n",
      " 37 Episode in  17 steps, reward 17.00\n",
      " 38 Episode in  22 steps, reward 22.00\n",
      " 39 Episode in  16 steps, reward 16.00\n",
      " 40 Episode in  31 steps, reward 31.00\n",
      " 41 Episode in  30 steps, reward 30.00\n",
      " 42 Episode in  28 steps, reward 28.00\n",
      " 43 Episode in  15 steps, reward 15.00\n",
      " 44 Episode in  29 steps, reward 29.00\n",
      " 45 Episode in  32 steps, reward 32.00\n",
      " 46 Episode in  25 steps, reward 25.00\n",
      " 47 Episode in  29 steps, reward 29.00\n",
      " 48 Episode in  18 steps, reward 18.00\n",
      " 49 Episode in  27 steps, reward 27.00\n",
      " 50 Episode in  18 steps, reward 18.00\n",
      " 51 Episode in  40 steps, reward 40.00\n",
      " 52 Episode in  39 steps, reward 39.00\n",
      " 53 Episode in  49 steps, reward 49.00\n",
      " 54 Episode in  19 steps, reward 19.00\n",
      " 55 Episode in  17 steps, reward 17.00\n",
      " 56 Episode in  11 steps, reward 11.00\n",
      " 57 Episode in  37 steps, reward 37.00\n",
      " 58 Episode in  12 steps, reward 12.00\n",
      " 59 Episode in  16 steps, reward 16.00\n",
      " 60 Episode in  36 steps, reward 36.00\n",
      " 61 Episode in  26 steps, reward 26.00\n",
      " 62 Episode in  26 steps, reward 26.00\n",
      " 63 Episode in  11 steps, reward 11.00\n",
      " 64 Episode in  16 steps, reward 16.00\n",
      " 65 Episode in  15 steps, reward 15.00\n",
      " 66 Episode in  18 steps, reward 18.00\n",
      " 67 Episode in  56 steps, reward 56.00\n",
      " 68 Episode in  31 steps, reward 31.00\n",
      " 69 Episode in  22 steps, reward 22.00\n",
      " 70 Episode in  26 steps, reward 26.00\n",
      " 71 Episode in  52 steps, reward 52.00\n",
      " 72 Episode in  21 steps, reward 21.00\n",
      " 73 Episode in  14 steps, reward 14.00\n",
      " 74 Episode in  28 steps, reward 28.00\n",
      " 75 Episode in  37 steps, reward 37.00\n",
      " 76 Episode in  16 steps, reward 16.00\n",
      " 77 Episode in  37 steps, reward 37.00\n",
      " 78 Episode in  10 steps, reward 10.00\n",
      " 79 Episode in  16 steps, reward 16.00\n",
      " 80 Episode in  13 steps, reward 13.00\n",
      " 81 Episode in  24 steps, reward 24.00\n",
      " 82 Episode in  19 steps, reward 19.00\n",
      " 83 Episode in  15 steps, reward 15.00\n",
      " 84 Episode in  21 steps, reward 21.00\n",
      "\n",
      "==========  Learning Start  ==========\n",
      " 85 Episode in  40 steps, reward 40.00\n",
      " 86 Episode in  18 steps, reward 18.00\n",
      " 87 Episode in  13 steps, reward 13.00\n",
      " 88 Episode in  26 steps, reward 26.00\n",
      " 89 Episode in  15 steps, reward 15.00\n",
      " 90 Episode in  31 steps, reward 31.00\n",
      " 91 Episode in  27 steps, reward 27.00\n",
      " 92 Episode in  32 steps, reward 32.00\n",
      " 93 Episode in  32 steps, reward 32.00\n",
      " 94 Episode in  21 steps, reward 21.00\n",
      " 95 Episode in  24 steps, reward 24.00\n",
      " 96 Episode in  11 steps, reward 11.00\n",
      " 97 Episode in  46 steps, reward 46.00\n",
      " 98 Episode in  23 steps, reward 23.00\n",
      " 99 Episode in  16 steps, reward 16.00\n",
      "100 Episode in  20 steps, reward 20.00\n",
      "101 Episode in  20 steps, reward 20.00\n",
      "102 Episode in  13 steps, reward 13.00\n",
      "103 Episode in  15 steps, reward 15.00\n",
      "104 Episode in  13 steps, reward 13.00\n",
      "105 Episode in  38 steps, reward 38.00\n",
      "106 Episode in  30 steps, reward 30.00\n",
      "107 Episode in  14 steps, reward 14.00\n",
      "108 Episode in  11 steps, reward 11.00\n",
      "109 Episode in  11 steps, reward 11.00\n",
      "110 Episode in  12 steps, reward 12.00\n",
      "111 Episode in  18 steps, reward 18.00\n",
      "112 Episode in  52 steps, reward 52.00\n",
      "113 Episode in  19 steps, reward 19.00\n",
      "114 Episode in  25 steps, reward 25.00\n",
      "115 Episode in  13 steps, reward 13.00\n",
      "116 Episode in  12 steps, reward 12.00\n",
      "117 Episode in  13 steps, reward 13.00\n",
      "118 Episode in  22 steps, reward 22.00\n",
      "119 Episode in  11 steps, reward 11.00\n",
      "120 Episode in  37 steps, reward 37.00\n",
      "121 Episode in  53 steps, reward 53.00\n",
      "122 Episode in  16 steps, reward 16.00\n",
      "123 Episode in  18 steps, reward 18.00\n",
      "124 Episode in  52 steps, reward 52.00\n",
      "125 Episode in  12 steps, reward 12.00\n",
      "126 Episode in  45 steps, reward 45.00\n",
      "127 Episode in  32 steps, reward 32.00\n",
      "128 Episode in  23 steps, reward 23.00\n",
      "129 Episode in  25 steps, reward 25.00\n",
      "130 Episode in  15 steps, reward 15.00\n",
      "131 Episode in  13 steps, reward 13.00\n",
      "132 Episode in  23 steps, reward 23.00\n",
      "133 Episode in  16 steps, reward 16.00\n",
      "134 Episode in  13 steps, reward 13.00\n",
      "135 Episode in  16 steps, reward 16.00\n",
      "136 Episode in  12 steps, reward 12.00\n",
      "137 Episode in  10 steps, reward 10.00\n",
      "138 Episode in  26 steps, reward 26.00\n",
      "139 Episode in  16 steps, reward 16.00\n",
      "140 Episode in  33 steps, reward 33.00\n",
      "141 Episode in  24 steps, reward 24.00\n",
      "142 Episode in  14 steps, reward 14.00\n",
      "143 Episode in  14 steps, reward 14.00\n",
      "144 Episode in  20 steps, reward 20.00\n",
      "145 Episode in  15 steps, reward 15.00\n",
      "146 Episode in  26 steps, reward 26.00\n",
      "147 Episode in  13 steps, reward 13.00\n",
      "148 Episode in  61 steps, reward 61.00\n",
      "149 Episode in  34 steps, reward 34.00\n",
      "150 Episode in   9 steps, reward 9.00\n",
      "151 Episode in   9 steps, reward 9.00\n",
      "152 Episode in  38 steps, reward 38.00\n",
      "153 Episode in  18 steps, reward 18.00\n",
      "154 Episode in  19 steps, reward 19.00\n",
      "155 Episode in  40 steps, reward 40.00\n",
      "156 Episode in  27 steps, reward 27.00\n",
      "157 Episode in  20 steps, reward 20.00\n",
      "158 Episode in  32 steps, reward 32.00\n",
      "159 Episode in  25 steps, reward 25.00\n",
      "160 Episode in  11 steps, reward 11.00\n",
      "161 Episode in  53 steps, reward 53.00\n",
      "162 Episode in  34 steps, reward 34.00\n",
      "163 Episode in  35 steps, reward 35.00\n",
      "164 Episode in  14 steps, reward 14.00\n",
      "165 Episode in  37 steps, reward 37.00\n",
      "166 Episode in  18 steps, reward 18.00\n",
      "167 Episode in  36 steps, reward 36.00\n",
      "168 Episode in  35 steps, reward 35.00\n",
      "169 Episode in  10 steps, reward 10.00\n",
      "170 Episode in  16 steps, reward 16.00\n",
      "171 Episode in  46 steps, reward 46.00\n",
      "172 Episode in  15 steps, reward 15.00\n",
      "173 Episode in  36 steps, reward 36.00\n",
      "174 Episode in  20 steps, reward 20.00\n",
      "175 Episode in  41 steps, reward 41.00\n",
      "176 Episode in  22 steps, reward 22.00\n",
      "177 Episode in  24 steps, reward 24.00\n",
      "178 Episode in  54 steps, reward 54.00\n",
      "179 Episode in  43 steps, reward 43.00\n",
      "180 Episode in  16 steps, reward 16.00\n",
      "181 Episode in  20 steps, reward 20.00\n",
      "182 Episode in  35 steps, reward 35.00\n",
      "183 Episode in  20 steps, reward 20.00\n",
      "184 Episode in  36 steps, reward 36.00\n",
      "185 Episode in  63 steps, reward 63.00\n",
      "186 Episode in  32 steps, reward 32.00\n",
      "187 Episode in  16 steps, reward 16.00\n",
      "188 Episode in  17 steps, reward 17.00\n",
      "189 Episode in  49 steps, reward 49.00\n",
      "190 Episode in 104 steps, reward 104.00\n",
      "191 Episode in  21 steps, reward 21.00\n",
      "192 Episode in  50 steps, reward 50.00\n",
      "193 Episode in  19 steps, reward 19.00\n",
      "194 Episode in  37 steps, reward 37.00\n",
      "195 Episode in  27 steps, reward 27.00\n",
      "196 Episode in  44 steps, reward 44.00\n",
      "197 Episode in  18 steps, reward 18.00\n",
      "198 Episode in  41 steps, reward 41.00\n",
      "199 Episode in  52 steps, reward 52.00\n",
      "200 Episode in  29 steps, reward 29.00\n",
      "201 Episode in  46 steps, reward 46.00\n",
      "202 Episode in  32 steps, reward 32.00\n",
      "203 Episode in  14 steps, reward 14.00\n",
      "204 Episode in  26 steps, reward 26.00\n",
      "205 Episode in  22 steps, reward 22.00\n",
      "206 Episode in  72 steps, reward 72.00\n",
      "207 Episode in  49 steps, reward 49.00\n",
      "208 Episode in  20 steps, reward 20.00\n",
      "209 Episode in  26 steps, reward 26.00\n",
      "210 Episode in  27 steps, reward 27.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 Episode in  19 steps, reward 19.00\n",
      "212 Episode in  25 steps, reward 25.00\n",
      "213 Episode in  98 steps, reward 98.00\n",
      "214 Episode in  17 steps, reward 17.00\n",
      "215 Episode in  41 steps, reward 41.00\n",
      "216 Episode in  33 steps, reward 33.00\n",
      "217 Episode in  80 steps, reward 80.00\n",
      "218 Episode in 103 steps, reward 103.00\n",
      "219 Episode in  33 steps, reward 33.00\n",
      "220 Episode in  31 steps, reward 31.00\n",
      "221 Episode in  61 steps, reward 61.00\n",
      "222 Episode in  67 steps, reward 67.00\n",
      "223 Episode in  23 steps, reward 23.00\n",
      "224 Episode in 120 steps, reward 120.00\n",
      "225 Episode in  57 steps, reward 57.00\n",
      "226 Episode in  36 steps, reward 36.00\n",
      "227 Episode in 134 steps, reward 134.00\n",
      "228 Episode in  47 steps, reward 47.00\n",
      "229 Episode in  20 steps, reward 20.00\n",
      "230 Episode in  37 steps, reward 37.00\n",
      "231 Episode in  26 steps, reward 26.00\n",
      "232 Episode in  24 steps, reward 24.00\n",
      "233 Episode in 145 steps, reward 145.00\n",
      "234 Episode in  98 steps, reward 98.00\n",
      "235 Episode in  54 steps, reward 54.00\n",
      "236 Episode in  47 steps, reward 47.00\n",
      "237 Episode in 128 steps, reward 128.00\n",
      "238 Episode in  75 steps, reward 75.00\n",
      "239 Episode in 171 steps, reward 171.00\n",
      "240 Episode in 173 steps, reward 173.00\n",
      "241 Episode in 207 steps, reward 207.00\n",
      "242 Episode in 121 steps, reward 121.00\n",
      "243 Episode in  30 steps, reward 30.00\n",
      "244 Episode in 190 steps, reward 190.00\n",
      "245 Episode in 116 steps, reward 116.00\n",
      "246 Episode in  17 steps, reward 17.00\n",
      "247 Episode in 194 steps, reward 194.00\n",
      "248 Episode in 177 steps, reward 177.00\n",
      "249 Episode in 427 steps, reward 427.00\n",
      "250 Episode in 187 steps, reward 187.00\n",
      "251 Episode in 168 steps, reward 168.00\n",
      "252 Episode in 325 steps, reward 325.00\n",
      "253 Episode in 215 steps, reward 215.00\n",
      "254 Episode in 482 steps, reward 482.00\n",
      "255 Episode in 298 steps, reward 298.00\n",
      "256 Episode in 351 steps, reward 351.00\n",
      "257 Episode in 345 steps, reward 345.00\n",
      "258 Episode in 271 steps, reward 271.00\n",
      "259 Episode in 313 steps, reward 313.00\n",
      "260 Episode in 413 steps, reward 413.00\n",
      "261 Episode in 270 steps, reward 270.00\n",
      "262 Episode in 217 steps, reward 217.00\n",
      "263 Episode in 240 steps, reward 240.00\n",
      "264 Episode in 252 steps, reward 252.00\n",
      "265 Episode in 429 steps, reward 429.00\n",
      "266 Episode in 322 steps, reward 322.00\n",
      "267 Episode in 290 steps, reward 290.00\n",
      "268 Episode in 176 steps, reward 176.00\n",
      "269 Episode in 277 steps, reward 277.00\n",
      "270 Episode in 392 steps, reward 392.00\n",
      "271 Episode in 332 steps, reward 332.00\n",
      "272 Episode in 232 steps, reward 232.00\n",
      "273 Episode in 332 steps, reward 332.00\n",
      "274 Episode in 351 steps, reward 351.00\n",
      "275 Episode in 341 steps, reward 341.00\n",
      "276 Episode in 398 steps, reward 398.00\n",
      "277 Episode in 222 steps, reward 222.00\n",
      "278 Episode in 394 steps, reward 394.00\n",
      "279 Episode in 228 steps, reward 228.00\n",
      "280 Episode in 410 steps, reward 410.00\n",
      "281 Episode in 221 steps, reward 221.00\n",
      "282 Episode in 254 steps, reward 254.00\n",
      "283 Episode in 276 steps, reward 276.00\n",
      "284 Episode in 194 steps, reward 194.00\n",
      "285 Episode in 281 steps, reward 281.00\n",
      "286 Episode in 500 steps, reward 500.00\n",
      "287 Episode in 451 steps, reward 451.00\n",
      "288 Episode in 290 steps, reward 290.00\n",
      "289 Episode in 241 steps, reward 241.00\n",
      "290 Episode in 500 steps, reward 500.00\n",
      "291 Episode in 221 steps, reward 221.00\n",
      "292 Episode in 275 steps, reward 275.00\n",
      "293 Episode in 184 steps, reward 184.00\n",
      "294 Episode in 380 steps, reward 380.00\n",
      "295 Episode in 486 steps, reward 486.00\n",
      "296 Episode in 255 steps, reward 255.00\n",
      "297 Episode in 149 steps, reward 149.00\n",
      "298 Episode in 215 steps, reward 215.00\n",
      "299 Episode in 219 steps, reward 219.00\n",
      "300 Episode in 291 steps, reward 291.00\n",
      "301 Episode in 414 steps, reward 414.00\n",
      "302 Episode in 216 steps, reward 216.00\n",
      "303 Episode in 164 steps, reward 164.00\n",
      "304 Episode in 241 steps, reward 241.00\n",
      "305 Episode in 338 steps, reward 338.00\n",
      "\n",
      "Cartpole is sloved! 305 Episode in 25597 steps\n"
     ]
    }
   ],
   "source": [
    "# play\n",
    "n_episodes = 500\n",
    "memory_size = 10000\n",
    "learn_start = 2000\n",
    "update_frq = 1\n",
    "epsilon = 1.0\n",
    "eps_min = 0.02\n",
    "total_steps = 0\n",
    "n_dones = 0\n",
    "rewards = []\n",
    "is_learned = False\n",
    "is_solved = False\n",
    "\n",
    "# make a replay memory\n",
    "rep_memory = deque(maxlen=memory_size)\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    ep_reward = 0\n",
    "    ep_steps = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        # epsilon greedy\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            target_net.eval()\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor([obs]).to(device).float()\n",
    "                probs = target_net(state)\n",
    "                weights = probs * net.support\n",
    "                q = weights.sum(dim=2)\n",
    "                q_np = q.cpu().numpy()[0]\n",
    "            action = np.argmax(q_np)\n",
    "\n",
    "        _obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        rep_memory.append((obs, action, reward, _obs, done))\n",
    "        \n",
    "        obs = _obs\n",
    "        ep_reward += reward\n",
    "        ep_steps += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        # tensorboard\n",
    "        if USE_TENSORBOARD:\n",
    "            writer.add_scalar('reward', ep_reward, i + 1)\n",
    "        \n",
    "        # learning\n",
    "        if len(rep_memory) >= learn_start:\n",
    "            if len(rep_memory) == learn_start:\n",
    "                is_learned = True\n",
    "                print('\\n==========  Learning Start  ==========')\n",
    "            train(net, target_net, rep_memory)\n",
    "\n",
    "            # epsilon decay\n",
    "            epsilon -= 1 / 10**4\n",
    "            epsilon = max(eps_min, epsilon)\n",
    "\n",
    "        if done:\n",
    "            rewards.append(ep_reward)\n",
    "            n_dones += 1\n",
    "            print('{:3} Episode in {:3} steps, reward {:.2f}'.format(i + 1, ep_steps, ep_reward))\n",
    "            if is_learned:\n",
    "                # sync target net\n",
    "                if n_dones % update_frq == 0:\n",
    "                    target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "            # evaluate\n",
    "            if len(rewards) > 20:\n",
    "                if np.mean(rewards[-21:-1]) >= 200:\n",
    "                    is_solved = True\n",
    "                    print('\\nCartpole is sloved! {:3} Episode in {:3} steps'.format(i + 1, total_steps))\n",
    "            break\n",
    "    \n",
    "    if is_solved:                  \n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
