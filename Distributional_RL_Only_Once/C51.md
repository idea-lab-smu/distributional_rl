본 readme는 Distributional_RL.py 내부의 클래스를 설명하는 문서입니다.

그 중에서 model이 C51일 경우에 대한 설명입니다. C51을 학습하는데 필요한 파라미터만 설명합니다. 용어에 대해서 애매모호한 부분은 [다음 링크](https://github.com/reinforcement-learning-kr/distributional_rl/tree/master/1_CartPole_C51)에서 참조하세요

# Distributional_RL.py

## import library

``` python
import gym
import tensorflow as tf
import numpy as np
import random
```

필요한 library들을 import합니다.

## __init__(self, sess, model, learning_rate)

에이전트를 생성하고 학습하는데 필요한 파라미터들을 설정합니다.

``` python
def __init__(self, sess, model, learning_rate):
        self.learning_rate = learning_rate
        self.state_size = 4
        self.action_size = 2
        self.model = model
        self.sess = sess
        self.batch_size = 8
        self.gamma = 0.99

        self.num_support = 8
        self.V_max = 5
        self.V_min = -5
        self.dz = float(self.V_max - self.V_min) / (self.num_support - 1)
        self.z = [self.V_min + i * self.dz for i in range(self.num_support)]

        self.state = tf.placeholder(tf.float32, [None, self.state_size])
        self.action = tf.placeholder(tf.float32, [None, self.action_size])
        self.M = tf.placeholder(tf.float32, [None, self.num_support])

        self.main_network, self.main_action_support, self.main_params = self._build_network('main')
        self.target_network, self.target_action_support, self.target_params = self._build_network('target')
```

self.learning_rate는 학습률이며, self.state_size는 state의 크기, self.action_size는 행동의 개수입니다. 여기서는 CartPole에 적용하였으므로 self.state_size = 4, self.action_size = 2입니다. self.model은 사용할 모델을 정의합니다. C51로 정의됩니다. self.sess는 tensorflow Session(tf.Session())입니다. self.batch_size는 실제 학습할 때 샘플링할 메모리의 개수입니다. self.num_support는 support의 개수입니다. self.V_max는 support의 최대값입니다. self.V_min은 support의 최소값입니다. self.dz는 support 한 칸의 넓이입니다. self.z는 support 하나하나가 가지는 값입니다. 위의 예제에서는 self.dz는 10/7이며 self.z는 [-5, -5+10/7, -5+2x10/7, ..., 5]입니다. Class 내부에 정의되어 있는 함수 _build_network를 통해 main network와 target network를 생성합니다. _build_network는 학습에 사용할 network, inference에 사용할 network, 그리고 network를 구성하고 있는 parameter를 출력으로 합니다.

``` python
elif self.model == 'C51':
        expand_dim_action = tf.expand_dims(self.action, -1)
        self.Q_s_a = self.main_network * expand_dim_action
        self.Q_s_a = tf.reduce_sum(self.Q_s_a, axis=1)
        self.loss = - tf.reduce_mean(tf.reduce_sum(tf.multiply(self.M, tf.log(self.Q_s_a + 1e-20)), axis=1))
        self.train_op = tf.train.AdamOptimizer(0.001).minimize(self.loss)
```

self.action을 학습을 가능하도록 dimension을 맞춰주기 위해서 expand_dims를 사용한 후 expand_dim_action에 정의합니다. self.main_network에서 나온 각 action에 해당하는 값과 expand_dim_action과 곱함으로써 실제 선택한 action에 대한 값만 분포만을 self.Q_s_a에 정의합니다. 그리고 계산한 [target distribution](https://github.com/reinforcement-learning-kr/distributional_rl/tree/master/1_CartPole_C51)과의 cross-entropy를 self.loss로 정의한 후 self.loss를 최소화하는 방향으로 학습하는 것을 self.train_op로 지정합니다.

# train(self, memory)

``` python
def train(self, memory):
        minibatch = random.sample(memory, self.batch_size)
        state_stack = [mini[0] for mini in minibatch]
        next_state_stack = [mini[1] for mini in minibatch]
        action_stack = [mini[2] for mini in minibatch]
        reward_stack = [mini[3] for mini in minibatch]
        done_stack = [mini[4] for mini in minibatch]
        done_stack = [int(i) for i in done_stack]
        elif self.model == 'C51':
            z_space = tf.tile(tf.reshape(self.z, [1, 1, self.num_support]), [self.batch_size, self.action_size, 1])
            prob_next_state = self.sess.run(self.target_network, feed_dict={self.state: next_state_stack})
            Q_next_state = self.sess.run(self.target_action_support * z_space, feed_dict={self.state: next_state_stack})
            next_action = np.argmax(np.sum(Q_next_state, axis=2), axis=1)
            prob_next_state_action = [prob_next_state[i, action, :] for i, action in enumerate(next_action)]

            m_prob = np.zeros([self.batch_size, self.num_support])

            for i in range(self.batch_size):
                for j in range(self.num_support):
                    Tz = np.fmin(self.V_max, np.fmax(self.V_min, reward_stack[i] + (1 - done_stack[i]) * 0.99 * (self.V_min + j * self.dz)))
                    bj = (Tz - self.V_min) / self.dz

                    lj = np.floor(bj).astype(int)
                    uj = np.ceil(bj).astype(int)

                    blj = bj - lj
                    buj = uj - bj

                    m_prob[i, lj] += (done_stack[i] + (1 - done_stack[i]) * (prob_next_state_action[i][j])) * buj
                    m_prob[i, uj] += (done_stack[i] + (1 - done_stack[i]) * (prob_next_state_action[i][j])) * blj

            m_prob = m_prob / m_prob.sum(axis=1, keepdims=1)

            return self.sess.run([self.train_op, self.loss],
                                 feed_dict={self.state: state_stack, self.action: action_stack, self.M: m_prob})
```

먼저 self.batch_size만큼 메모리에서 샘플들을 뽑습니다. 그 후 state_stack, next_state_stack, action_stack. reward_stack, done_stack을 새로 지정한 후 배치합니다. prob_next_state는 next_state_stack에 대한 출력으로 Qtarget(s')을 뜻합니다. 이에 대해 argmax(Qtarget(s'))를 통해 next_action(a')를 구한 후 Qtarget(s', a')을 prob_next_state_action에 지정합니다. m_prob = np.zeros([self.batch_size, self.num_support])부터 m_prob = m_prob / m_prob.sum(axis=1, keepdims=1)까지 histogram에 대해 [target distribution]((https://github.com/reinforcement-learning-kr/distributional_rl/tree/master/1_CartPole_C51)을 구한 후 m_prob에 지정합니다. 다음 self.M에 m_prob을 입력으로 하고 main network을 target distribution에 수렴시키는 training을 return 합니다.

# _build_network(self, name)

네트워크를 만드는 함수입니다.

``` python
def _build_network(self, name):
        with tf.variable_scope(name):
            elif self.model == 'C51':
                layer_1 = tf.layers.dense(inputs=self.state, units=64, activation=tf.nn.relu, trainable=True)
                layer_2 = tf.layers.dense(inputs=layer_1, units=64, activation=tf.nn.relu, trainable=True)
                layer_3 = tf.layers.dense(inputs=layer_2, units=self.action_size * self.num_support, activation=None,
                                          trainable=True)

                net_pre = tf.reshape(layer_3, [-1, self.action_size, self.num_support])
                net = tf.nn.softmax(net_pre, axis=2)
                net_action = net

        params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)

        return net, net_action, params
```

네트워크를 만듭니다. 그리고 net은 학습을 위한 network, net_action은 inference를 위한 network이며 이를 구성하고 있는 params를 return합니다. C51에서는 학습을 위한 network와 inference를 위한 network가 분리될 필요가 없기에 net_action = net을 사용하여 같게 두었습니다.

# choose_action(self, state)

network을 이용하여 action을 inference하는 함수입니다.

``` python
def _build_network(self, name):
        elif self.model == 'C51':
            Q = self.sess.run(self.main_action_support, feed_dict={self.state: [state]})
            z_space = np.repeat(np.expand_dims(self.z, axis=0), self.action_size, axis=0)
            Q_s_a = np.sum(Q[0] * z_space, axis=1)
            action = np.argmax(Q_s_a)

        return action
```

result는 main_network에서 state을 입력으로하여 Q(s)를 구합니다. 이 histogram에 대해 평균을 구하여 Q_s_a를 비교한 후 이 중에 큰 action을 선택합니다.